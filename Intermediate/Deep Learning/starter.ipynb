{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Intermediate Level\n",
    "\n",
    "Welcome to the Deep Learning intermediate tasks! This notebook contains three comprehensive tasks to test your understanding of neural networks, transfer learning, and regularization.\n",
    "\n",
    "## Tasks Overview:\n",
    "1. **Task 1: Neural Network from Scratch** - Build a neural network using only NumPy\n",
    "2. **Task 2: Transfer Learning** - Fine-tune pre-trained models\n",
    "3. **Task 3: Regularization Techniques** - Prevent overfitting with various techniques\n",
    "\n",
    "Please refer to `tasks.md` for detailed requirements for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import necessary libraries\n",
    "# Example:\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Neural Network from Scratch\n",
    "\n",
    "Build a simple feedforward neural network from scratch using only NumPy.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement forward propagation\n",
    "- Implement backpropagation with gradient descent\n",
    "- Train on a simple dataset (e.g., XOR problem or MNIST subset)\n",
    "- Plot the loss curve over epochs\n",
    "- Achieve convergence and demonstrate learning\n",
    "\n",
    "**Hints:**\n",
    "- Implement activation functions (sigmoid, ReLU, softmax)\n",
    "- Use matrix operations for efficient computation\n",
    "- Initialize weights carefully (e.g., Xavier initialization)\n",
    "- Start with a simple 2-layer network, then expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 1 - Implement activation functions\n",
    "# Implement sigmoid, ReLU, and their derivatives\n",
    "# These will be used in forward and backward propagation\n",
    "def sigmoid(X):\n",
    "    return 1/(1 + np.exp(-X))\n",
    "\n",
    "def sigmoid_d(X):\n",
    "    return sigmoid(X) * (1 - sigmoid(X))\n",
    "\n",
    "def relu(X):\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "def relu_d(X):\n",
    "    return np.greater(X, 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 0., 1., 1., 0., 1.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_d(np.array([1, 2, 3, -1, -3, 3, 5, -5, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_weights\u001b[39m(n_input, n_output):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn((n_input, n_output), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39mn_input)\n\u001b[1;32m----> 7\u001b[0m \u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[54], line 5\u001b[0m, in \u001b[0;36minit_weights\u001b[1;34m(n_input, n_output)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_weights\u001b[39m(n_input, n_output):\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mn_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:1310\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randn\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:1471\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:657\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# TODO: Step 2 - Initialize network parameters\n",
    "# Create weight matrices and bias vectors\n",
    "# Use appropriate initialization (Xavier, He, etc.)\n",
    "def init_weights(n_input, n_output):\n",
    "    return np.random.randn((n_input, n_output), 0, 2/n_input)\n",
    "\n",
    "init_weights(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 3 - Implement forward propagation\n",
    "# Compute layer outputs from input to output\n",
    "# Store intermediate values for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 4 - Implement backpropagation\n",
    "# Compute gradients using chain rule\n",
    "# Update weights and biases using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 5 - Load dataset and train\n",
    "# Load XOR dataset or MNIST subset\n",
    "# Train your network for multiple epochs\n",
    "# Track loss at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 6 - Plot learning curve\n",
    "# Visualize loss over epochs\n",
    "# Demonstrate that the network is learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Transfer Learning\n",
    "\n",
    "Use transfer learning with a pre-trained model (e.g., ResNet, VGG, or MobileNet) for a custom classification task.\n",
    "\n",
    "**Requirements:**\n",
    "- Load a pre-trained model and freeze base layers\n",
    "- Add custom classification layers\n",
    "- Fine-tune on a new dataset (at least 3 classes)\n",
    "- Compare performance with and without transfer learning\n",
    "- Visualize feature maps from different layers\n",
    "\n",
    "**Hints:**\n",
    "- Use pre-trained models from popular frameworks\n",
    "- Freeze early layers, train only top layers first\n",
    "- Optionally unfreeze and fine-tune later layers\n",
    "- Use appropriate data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 1 - Load dataset\n",
    "# Choose a dataset with at least 3 classes\n",
    "# Could use a subset of ImageNet, custom dataset, etc.\n",
    "# Split into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 2 - Load pre-trained model\n",
    "# Load model from keras.applications (ResNet50, VGG16, MobileNetV2, etc.)\n",
    "# Remove top layers (include_top=False)\n",
    "# Freeze base model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 3 - Add custom classification layers\n",
    "# Add GlobalAveragePooling, Dense layers\n",
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 4 - Train with transfer learning\n",
    "# Train only the top layers first\n",
    "# Monitor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 5 - Train baseline model from scratch\n",
    "# Build and train same architecture without pre-trained weights\n",
    "# Compare performance with transfer learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 6 - Visualize feature maps\n",
    "# Extract intermediate layer outputs\n",
    "# Visualize learned features from different layers\n",
    "# Compare low-level vs high-level features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Regularization Techniques\n",
    "\n",
    "Implement and compare different regularization techniques to prevent overfitting.\n",
    "\n",
    "**Requirements:**\n",
    "- Create a baseline model on a dataset of your choice\n",
    "- Implement dropout regularization\n",
    "- Implement L1/L2 weight regularization\n",
    "- Apply batch normalization\n",
    "- Compare validation performance across all approaches\n",
    "- Visualize overfitting reduction\n",
    "\n",
    "**Hints:**\n",
    "- Use a dataset prone to overfitting (small dataset or complex model)\n",
    "- Apply one technique at a time to see individual effects\n",
    "- Monitor train vs validation metrics\n",
    "- Try combining multiple techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 1 - Create baseline model\n",
    "# Build a model that overfits (intentionally complex for the data)\n",
    "# Train without any regularization\n",
    "# Observe the gap between training and validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 2 - Implement Dropout\n",
    "# Add Dropout layers to the model\n",
    "# Train and compare with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 3 - Implement L1/L2 Regularization\n",
    "# Add kernel_regularizer to layers\n",
    "# Try L1, L2, and L1_L2 regularizers\n",
    "# Train and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 4 - Implement Batch Normalization\n",
    "# Add BatchNormalization layers after Dense/Conv layers\n",
    "# Train and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 5 - Compare all approaches\n",
    "# Create comparison plots showing:\n",
    "# - Training vs validation loss for each approach\n",
    "# - Final accuracies for each approach\n",
    "# - Discussion of which techniques work best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
